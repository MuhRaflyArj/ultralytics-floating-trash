{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "from ultralytics.models.yolo.model import DetectionModel # Ensure this import is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Configuration ---\n",
    "CONFIG_NAME = 'yolo11s-dspan.yaml' # Name of the YAML file for the modified LCBHAM model\n",
    "# File Paths\n",
    "current_dir = os.getcwd()\n",
    "target_yaml_path = os.path.join(current_dir, 'ultralytics', 'cfg', 'models', '11', CONFIG_NAME)\n",
    "source_weights_path = os.path.join(current_dir, 'yolo11s.pt')           # Pre-trained standard YOLOv11s\n",
    "output_weights_path = os.path.join(current_dir, 'runs', 'models', f'{CONFIG_NAME.split(\".\")[0]}.pt') # Output path for transferred weights\n",
    "\n",
    "# Data Paths\n",
    "data_path = os.path.join(current_dir, 'datasets', 'AblationDataset', 'wt-flow-taco', 'wt-flow-taco.yaml')\n",
    "test_data_path = os.path.join(current_dir, 'datasets', 'TestDataset', 'flowtest', 'flowtest.yaml')\n",
    "test_data_path_wt = os.path.join(current_dir, 'datasets', 'TestDataset', 'watertrashtest', 'watertrashtest.yaml')\n",
    "\n",
    "# Model Parameters (Ensure these match your dataset and model)\n",
    "nc = 1 # Number of classes (adjust if your dataset differs from COCO)\n",
    "\n",
    "# LCBHAM Layer Indices (Confirm these match your target_yaml_path)\n",
    "lcbham_layers_indices = {17}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Perform Weight Transfer (if necessary) ---\n",
    "\n",
    "if not os.path.exists(output_weights_path):\n",
    "    print(f\"Transfer weights file '{output_weights_path}' not found. Performing weight transfer...\")\n",
    "\n",
    "    # --- Load Source Weights ---\n",
    "    print(f\"Loading source weights from {source_weights_path}...\")\n",
    "    try:\n",
    "        source_ckpt = torch.load(source_weights_path, map_location=torch.device('cpu'), weights_only=False)\n",
    "\n",
    "        if hasattr(source_ckpt.get('model'), 'state_dict'):\n",
    "            source_state_dict = source_ckpt['model'].float().state_dict()\n",
    "        elif isinstance(source_ckpt.get('model'), dict):\n",
    "             source_state_dict = source_ckpt['model']\n",
    "        elif isinstance(source_ckpt, dict) and not 'model' in source_ckpt:\n",
    "             source_state_dict = source_ckpt\n",
    "        else:\n",
    "            print(\"Available keys in source_ckpt:\", source_ckpt.keys() if isinstance(source_ckpt, dict) else \"Not a dict\")\n",
    "            raise ValueError(\"Could not extract state_dict from source checkpoint.\")\n",
    "        print(f\"Source weights loaded and state_dict extracted.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading source weights: {e}\")\n",
    "        exit()\n",
    "\n",
    "    # --- Build Target Model Structure ---\n",
    "    print(f\"Building target model structure from {target_yaml_path}...\")\n",
    "    try:\n",
    "        # Ensure your custom LCBHAM module is defined/imported before this line\n",
    "        target_model = DetectionModel(cfg=str(target_yaml_path), ch=3, nc=nc)\n",
    "    except Exception as e:\n",
    "         print(f\"Error building target model: {e}\")\n",
    "         print(\"Ensure LCBHAM is defined, the YAML path is correct, and nc={nc} is appropriate.\")\n",
    "         exit()\n",
    "    target_state_dict = target_model.state_dict()\n",
    "    print(f\"Target model structure built.\")\n",
    "\n",
    "    # --- Weight Transfer Logic ---\n",
    "    print(\"Starting weight transfer...\")\n",
    "    new_state_dict = {}\n",
    "    transferred_count = 0\n",
    "    skipped_count = 0\n",
    "    lcbham_conv_transferred = set()\n",
    "    lcbham_bn_transferred = set()\n",
    "\n",
    "    for k_target, v_target in target_state_dict.items():\n",
    "        k_source = k_target\n",
    "        layer_index_str = k_target.split('.')[1] # Get layer index string like '17'\n",
    "\n",
    "        if layer_index_str.isdigit() and int(layer_index_str) in lcbham_layers_indices:\n",
    "            layer_index = int(layer_index_str)\n",
    "            if f\".{layer_index_str}.conv_block.0.\" in k_target: # Conv part of LCBHAM\n",
    "                k_source = k_target.replace(\"conv_block.0.\", \"conv.\")\n",
    "                lcbham_conv_transferred.add(layer_index)\n",
    "            elif f\".{layer_index_str}.conv_block.1.\" in k_target: # BN part of LCBHAM\n",
    "                k_source = k_target.replace(\"conv_block.1.\", \"bn.\")\n",
    "                lcbham_bn_transferred.add(layer_index)\n",
    "\n",
    "        if k_source in source_state_dict and source_state_dict[k_source].shape == v_target.shape:\n",
    "            new_state_dict[k_target] = source_state_dict[k_source]\n",
    "            transferred_count += 1\n",
    "        else:\n",
    "            new_state_dict[k_target] = v_target\n",
    "            skipped_count += 1\n",
    "\n",
    "    print(\"\\n--- Transfer Summary ---\")\n",
    "    print(f\"Total keys in target model: {len(target_state_dict)}\")\n",
    "    print(f\"Weights transferred: {transferred_count}\")\n",
    "    print(f\"Weights skipped/kept from target: {skipped_count}\")\n",
    "    for idx in lcbham_layers_indices:\n",
    "        if idx in lcbham_conv_transferred: print(f\"Successfully mapped Conv weights for LCBHAM layer {idx}.\")\n",
    "        if idx in lcbham_bn_transferred: print(f\"Successfully mapped BN weights for LCBHAM layer {idx}.\")\n",
    "\n",
    "    # --- Load New State Dict and Save Checkpoint ---\n",
    "    target_model.load_state_dict(new_state_dict, strict=False)\n",
    "    print(\"\\nLoaded transferred weights into target model structure.\")\n",
    "\n",
    "    output_ckpt = {\n",
    "        'epoch': -1,\n",
    "        'best_fitness': None,\n",
    "        'model': target_model,\n",
    "        'ema': None,\n",
    "        'updates': None,\n",
    "        'train_args': {}, # Use empty dict for compatibility\n",
    "        'date': None\n",
    "}\n",
    "\n",
    "    # Get the directory part of the output path using os.path\n",
    "    output_directory = os.path.dirname(output_weights_path)\n",
    "\n",
    "    # Create the directory recursively, ignoring errors if it exists\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    # --- End Replacement Code ---\n",
    "\n",
    "    # Save the checkpoint (this line remains unchanged)\n",
    "    torch.save(output_ckpt, output_weights_path)\n",
    "    print(f\"Saved model with transferred weights to {output_weights_path}\")\n",
    "    print(\"\\nWeight transfer complete.\")\n",
    "\n",
    "else:\n",
    "    print(f\"Found existing transfer weights file: '{output_weights_path}'. Skipping transfer step.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Fine-Tuning ---\n",
    "print(\"\\n--- Starting Fine-Tuning ---\")\n",
    "\n",
    "# Load the model with transferred weights\n",
    "model = YOLO(output_weights_path) # Load the result of the transfer\n",
    "# Train the model\n",
    "print(f\"Training on data: {data_path}\")\n",
    "\n",
    "# --- Train Model ---\n",
    "results = model.train(\n",
    "    # ** Essential Paths & Config **\n",
    "    data=str(data_path),                # Path to your dataset YAML file\n",
    "    name='wt-flow-taco-uav',                 # Name for the training run directory\n",
    "    exist_ok=False,                # Error if run name already exists\n",
    "    save=True,                     # Save checkpoints and final model\n",
    "\n",
    "    # ** Requested Hyperparameters **\n",
    "    epochs=300,                    # Number of training epochs\n",
    "    batch=16,                       # Batch size\n",
    "    imgsz=640,                     # Input image size (height=width=640)\n",
    "\n",
    "    # ** Data Augmentation Control (ONLY MOSAIC ENABLED) **\n",
    "    augment=False,                  # MUST be True to enable the augmentation pipeline for mosaic\n",
    ")\n",
    "print(\"\\n--- Training Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Validation ---\n",
    "print(\"\\n--- Starting Validation ---\")\n",
    "\n",
    "print(f\"Validating on flowtest: {test_data_path}\")\n",
    "try:\n",
    "    res_flow = model.val(data=str(test_data_path)) # Use the trained model object\n",
    "    print(\"Validation Results (flowtest):\")\n",
    "    # print(res_flow)\n",
    "except Exception as e:\n",
    "    print(f\"Error during validation on {test_data_path}: {e}\")\n",
    "\n",
    "\n",
    "print(f\"\\nValidating on watertrashtest: {test_data_path_wt}\")\n",
    "try:\n",
    "    res_wt = model.val(data=str(test_data_path_wt)) # Use the trained model object\n",
    "    print(\"Validation Results (watertrashtest):\")\n",
    "    # print(res_wt)\n",
    "except Exception as e:\n",
    "    print(f\"Error during validation on {test_data_path_wt}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
